{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary modules\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch \n",
    "import sys\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os, shutil\n",
    "from shutil import copyfile\n",
    "import copy\n",
    "from PIL import Image\n",
    "import csv\n",
    "import random\n",
    "print(\"All necessary modules imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_Name, num_classes, feature_extract, dict_path=None):\n",
    "    print(\"dict_path is None: \" + str(dict_path==None))\n",
    "    if model_Name == \"SqueezeNet\":\n",
    "        model = torchvision.models.squeezenet1_0(pretrained=False)\n",
    "        PATH = dict_path + \"/squeezenet1_0-a815701f.pth\"\n",
    "        if (dict_path != None):\n",
    "            model.load_state_dict(torch.load(PATH))\n",
    "            print(\"Pretrained model successfully loaded.\")\n",
    "#         set_parameters_that_require_grad(model, feature_extract)\n",
    "        set_parameters_that_require_grad(model, feature_extract)\n",
    "        model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model.num_classes = num_classes\n",
    "        input_size = 224\n",
    "    elif model_Name == \"mobilenet_v2\":\n",
    "        model = torchvision.models.mobilenet_v2(pretrained=False)\n",
    "        PATH = dict_path + \"/mobilenet_v2-b0353104.pth\"\n",
    "        if (dict_path != None):\n",
    "            model.load_state_dict(torch.load(PATH))\n",
    "            print(\"Pretrained model successfully loaded.\")\n",
    "        set_parameters_that_require_grad(model, feature_extract)        \n",
    "        model.classifier[1] = nn.Linear(1280, num_classes)\n",
    "        model.num_classes = num_classes\n",
    "        input_size = 224\n",
    "    elif model_Name == \"resnet50\":\n",
    "        model = torchvision.models.resnet50(pretrained=False)\n",
    "        PATH = dict_path + \"/resnet50-19c8e357.pth\"\n",
    "        if (dict_path != None):\n",
    "            model.load_state_dict(torch.load(PATH))\n",
    "            print(\"Pretrained model successfully loaded.\")\n",
    "        set_parameters_that_require_grad(model, feature_extract)\n",
    "        # reshaping the network\n",
    "        num_in_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_in_features, num_classes) # adding Linear layer at the end\n",
    "        input_size = 224\n",
    "    elif model_Name == \"inception_v3\": ## doesn't load!\n",
    "#       Inception v3\n",
    "#       Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        model = torchvision.models.inception_v3(pretrained=False)\n",
    "        PATH = dict_path + \"/inception_v3_google-1a9a5a14.pth\"\n",
    "        print(\"Starting to load...\")\n",
    "        if (dict_path != None):\n",
    "            model.load_state_dict(torch.load(PATH))\n",
    "            print(\"Pretrained model successfully loaded.\")\n",
    "        input_size = 299\n",
    "    elif model_Name == \"vgg11_bn\":\n",
    "        model = torchvision.models.vgg11_bn(pretrained=False)\n",
    "        PATH = dict_path + \"/vgg11_bn-6002323d.pth\"\n",
    "        if (dict_path != None):\n",
    "            model.load_state_dict(torch.load(PATH))\n",
    "            print(\"Pretrained model successfully loaded.\")\n",
    "        set_parameters_that_require_grad(model, feature_extract)\n",
    "        num_in_features = model.classifier[6].in_features\n",
    "        model.classifier[6] = nn.Linear(num_in_features, num_classes)\n",
    "        input_size = 224\n",
    "    else:\n",
    "        raise Exception(\"Model not recognized. Exiting.\")\n",
    "    return model, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameters_that_require_grad(model, feature_extract):\n",
    "    if feature_extract: # if we're in feature extracting mode\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False # freeze pretrained model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function loading labeled dataset (expecting labelling structure)\n",
    "def load_dataset(data_path, transforms):\n",
    "    data_set = ImageFolder(data_path, transforms)\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.transform:\n",
    "            x = self.transform(dataset[index][0])\n",
    "        else:\n",
    "            x = dataset[index][0]\n",
    "        y = dataset[index][1]\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "\n",
    "        return len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  training function  ##\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    valResultsPerEpoch = {}\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        print(\"epoch #\", epoch)\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10) \n",
    "        \n",
    "        valResultsPerEpoch[epoch] = []\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in tqdm(['train', 'val']):\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            # Iterate over data.\n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an\n",
    "                    # auxiliary output. In train mode we calculate the loss by\n",
    "                    # summing the final output and the auxiliary output but in\n",
    "                    # testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        print(\"(Not Inception)\")\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    \n",
    "                    ####\n",
    "                    if phase == 'val':\n",
    "                        print(\"preds-labels:\")\n",
    "                        diff = torch.abs(preds-labels) # returns a tensor! of absolute value of differences\n",
    "                        diff_list = diff.tolist() # now a list\n",
    "                        valResultsPerEpoch[epoch].extend(diff_list)\n",
    "                        print(\"DONE.\")\n",
    "                    ####\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                # statistics\n",
    "                # addition for each dataloader batch\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "                \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_epoch = epoch\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "                \n",
    "        print()\n",
    "        \n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best epoch was epoch #', best_epoch)\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))   \n",
    "    \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history, valResultsPerEpoch, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function returns the number of files in the subdirectory structure ###\n",
    "def howManyFiles(path):\n",
    "    notebook_path = os.path.dirname(os.path.realpath('__file__'))\n",
    "    rootDir = notebook_path + path\n",
    "    subDirs = os.listdir(rootDir)\n",
    "    count = 0\n",
    "    for sub in subDirs:\n",
    "        files = os.listdir(rootDir + sub)\n",
    "        count += len(files)\n",
    "    print(path, \" contains \", count, \" images.\")\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "howManyFiles('/GoogleStreetView_images/labelled_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_it(hist, num_epochs):\n",
    "    ohist = []\n",
    "    ohist = [h.cpu().numpy() for h in hist]\n",
    "    \n",
    "    plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
    "    plt.xlabel(\"Training Epochs\")\n",
    "    plt.ylabel(\"Validation Accuracy\")\n",
    "    plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "    plt.ylim((0,1.))\n",
    "    plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "    plt.legend()\n",
    "    plt.savefig('validation_accuracy_graph.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # proxy of the main function to be implemented\n",
    "    phoneData_dir = \"./Rogers_Corpus\"\n",
    "    data360_dir   = \"./CV-Aid-for-Visually-Impaired/data_processing/labelled_360/labelled_data\"\n",
    "    streetViewData_dir = \"./GoogleStreetView_images/labelled_data_already_scp-ed\"\n",
    "\n",
    "    # Ratio of training data to validation data\n",
    "    train_ratio = 0.9\n",
    "\n",
    "    # Number of classes in the dataset (class 0 being unknown)\n",
    "    num_classes = 8 # for google street view data\n",
    "\n",
    "    # Batch size for training (change depending on how much memory you have)\n",
    "    batch_size = 64\n",
    "\n",
    "    # Number of epochs to train for\n",
    "    num_epochs = 20\n",
    "\n",
    "    # Flag for feature extracting. When False, we finetune the whole model,\n",
    "    # when True we only update the reshaped (final) layer params\n",
    "    feature_extract = False\n",
    "\n",
    "    # Hyperparameters for models (learning rate and momentum)\n",
    "    learning_rate = 0.001\n",
    "    momentum = 0.9\n",
    "\n",
    "    # enter desired pretrained model\n",
    "    modelName = \"mobilenet_v2\"\n",
    "\n",
    "    modelX, input_size = load_model(modelName, num_classes, feature_extract, \"./raw_pretrained_models\")\n",
    "    print(\"Model imported.\")\n",
    "    \n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        # normalize using ImageNet's mean & standard deviation values\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    print(\"Initializing Datasets and Dataloaders...\")\n",
    "    \n",
    "    # creating training and validation datasets\n",
    "    print(\"input size: \", input_size)\n",
    "    \n",
    "    streetView_data_set = load_dataset(streetViewData_dir, data_transform) # 8 labels\n",
    "    print(\"-----------\")\n",
    "    print(streetView_data_set)\n",
    "    print(streetView_data_set.class_to_idx)    \n",
    "    print(\"streetView_data_set size: \", len(streetView_data_set))\n",
    "    \n",
    "    train_sizeStreetView = int(train_ratio * len(streetView_data_set))\n",
    "    val_sizeStreetView = len(streetView_data_set) - train_sizeStreetView\n",
    "    training_StreetView, val_StreetView = torch.utils.data.random_split(streetView_data_set,\n",
    "                                                                        [train_sizeStreetView, val_sizeStreetView])\n",
    "    \n",
    "    print(\"length training_StreetView: \", len(training_StreetView), \" images\")\n",
    "    print(type(training_StreetView))\n",
    "    print(\"length val_StreetView: \",len(val_StreetView), \" images\")\n",
    "    print(type(val_StreetView))\n",
    "    \n",
    "    imageStreetView_datasets = {'train': training_StreetView, 'val': val_StreetView }\n",
    "        \n",
    "    dataloadersStreetView_dict = {x: DataLoader(imageStreetView_datasets[x], batch_size=batch_size, shuffle=True) \n",
    "                                  for x in ['train', 'val']}\n",
    "\n",
    "    print(dataloadersStreetView_dict['train'])\n",
    "    print(\"length of trainStreetView dataloader: \", len(dataloadersStreetView_dict['train']), \" batches\") \n",
    "    \n",
    "    modelX = modelX.to(device)\n",
    "\n",
    "    params_to_update = modelX.parameters()\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "        for name, param in modelX.named_parameters():\n",
    "            if param.requires_grad is True:\n",
    "                params_to_update.append(param)\n",
    "                print(\"\\t\", name)\n",
    "    else:\n",
    "        for name, param in modelX.named_parameters():\n",
    "            if param.requires_grad is True:\n",
    "                print(\"\\t\", name)\n",
    "\n",
    "    # Observe that all parameters are being optimized\n",
    "    optimizerX = optim.SGD(params_to_update, lr=learning_rate, momentum=momentum)\n",
    "    \n",
    "    # Setup the loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Train and evaluate\n",
    "    modelX, hist, valResultsPerEpoch, bestEpoch = train_model(modelX, dataloadersStreetView_dict, criterion,\n",
    "                                 optimizerX, num_epochs=num_epochs,\n",
    "                                 is_inception=(modelName == \"inception\"))\n",
    "    ## modelX is now the best trained model\n",
    "    \n",
    "    print(\"best epoch was: \", bestEpoch)\n",
    "    print(\"valResultsPerEpoch for this epoch:\")\n",
    "    print(valResultsPerEpoch[bestEpoch])\n",
    "    \n",
    "    torch.save(modelX, \"./bestModel-Jul09.pth\")\n",
    "\n",
    "    \n",
    "    plot_it(hist, num_epochs)\n",
    "     \n",
    "#     sys.stdout.close()\n",
    "#     plot_it(hist, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(modelX, \"model\")      \n",
    "# plot_it(hist, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check best trained model on a new random validation set ###\n",
    "streetViewData_dir = \"./GoogleStreetView_images/labelled_data_already_scp-ed\"\n",
    "\n",
    "# Ratio of training data to validation data\n",
    "train_ratio = 0.9\n",
    "\n",
    "# Number of classes in the dataset (class 0 being unknown)\n",
    "num_classes = 8 # for google street view data\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 64\n",
    "input_size = 224\n",
    "\n",
    "notebook_path = os.path.dirname(os.path.realpath('__file__'))\n",
    "path = notebook_path + '/bestModel-Jul09.pth'\n",
    "print(path)\n",
    "# model = torch.load(path, map_location='cpu')\n",
    "model2_7 = torch.load(path)\n",
    "print(model)\n",
    "print(\"Model imported.\")\n",
    "\n",
    "##############\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(input_size),\n",
    "    transforms.ToTensor(),\n",
    "    # normalize using ImageNet's mean & standard deviation values\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "print(\"input size: \", input_size)\n",
    "\n",
    "streetView_data_set = load_dataset(streetViewData_dir, data_transform) # 8 labels\n",
    "print(\"-----------\")\n",
    "print(streetView_data_set)\n",
    "print(streetView_data_set.class_to_idx)\n",
    "\n",
    "print(\"streetView_data_set size: \", len(streetView_data_set))\n",
    "\n",
    "train_sizeStreetView = int(train_ratio * len(streetView_data_set))\n",
    "val_sizeStreetView = len(streetView_data_set) - train_sizeStreetView\n",
    "\n",
    "training_StreetView, val_StreetView = torch.utils.data.random_split(streetView_data_set,\n",
    "                                                                    [train_sizeStreetView, val_sizeStreetView])\n",
    "\n",
    "print(\"length training_StreetView: \", len(training_StreetView), \" images\")\n",
    "print(type(training_StreetView))\n",
    "print(\"length val_StreetView: \",len(val_StreetView), \" images\")\n",
    "print(type(val_StreetView))\n",
    "\n",
    "imageStreetView_datasets = {'train': training_StreetView, 'val': val_StreetView }\n",
    "\n",
    "\n",
    "dataloadersStreetView_dict = {x: DataLoader(imageStreetView_datasets[x], batch_size=batch_size, shuffle=True) \n",
    "                              for x in ['train', 'val']}\n",
    "\n",
    "print(dataloadersStreetView_dict['train'])\n",
    "print(\"length of trainStreetView dataloader: \", len(dataloadersStreetView_dict['train']), \" batches\")\n",
    "print(\"length of valStreetView dataloader: \", len(dataloadersStreetView_dict['val']), \" batches\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check best trained model on a new random validation set ###\n",
    "arr_preds = []\n",
    "arr_labels = []\n",
    "k = 0\n",
    "running_corrects = 0\n",
    "model2_7.eval()\n",
    "\n",
    "for inputs, labels in dataloadersStreetView_dict['val']:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = model2_7(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        arr_preds.append(preds.tolist())\n",
    "        arr_labels.append(labels.tolist())\n",
    "\n",
    "        if k < 3:\n",
    "            print(\"predictions:\", preds)\n",
    "            print(\"labels: \", labels)\n",
    "        k += 1\n",
    "\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "print(\"good predictions: \", running_corrects)\n",
    "accuracy = running_corrects.double() / len(dataloadersStreetView_dict['val'].dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check best trained model on a new random validation set ###\n",
    "accuracy = running_corrects.double() / len(dataloadersStreetView_dict['val'].dataset)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check best trained model on a new random validation set ###\n",
    "arr_preds_bis = []\n",
    "arr_labels_bis = [] \n",
    "print(len(arr_preds[0]))\n",
    "print(len(arr_labels))\n",
    "print(\"**\")\n",
    "# print(arr_preds[0])\n",
    "print(\"**\")\n",
    "# print(arr_preds)\n",
    "print(arr_labels)\n",
    "# print(arr_preds[0][1])\n",
    "print(\"----\")\n",
    "for k in range(10):\n",
    "    for i in range(len(arr_preds[k])):\n",
    "        arr_preds_bis.append(arr_preds[k][i])\n",
    "\n",
    "for k in range(10):\n",
    "    for i in range(len(arr_labels[k])):\n",
    "        arr_labels_bis.append(arr_labels[k][i])\n",
    "# print(arr_preds_bis)\n",
    "print(arr_labels_bis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check best trained model on a new random validation set ###\n",
    "notebook_path = os.path.dirname(os.path.realpath('__file__'))\n",
    "path = notebook_path + '/bestModel-Jul09.pth'\n",
    "print(path)\n",
    "model = torch.load(path)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "dummy_input = torch.randn(1, 3, 224, 224, device=device)\n",
    "# print(dummy_input)\n",
    "input_names = [\"input\"]\n",
    "output_names = [\"output\"]\n",
    "torch.onnx.export(model, dummy_input, \"bestModel2_onnx.onnx\", verbose=True, \n",
    "                  input_names=input_names, output_names=output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
