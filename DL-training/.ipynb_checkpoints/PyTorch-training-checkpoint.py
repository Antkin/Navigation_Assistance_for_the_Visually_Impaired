{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary modules\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch \n",
    "import sys\n",
    "print(torch.__version__)\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "print(\"Torchvision Version: \", torchvision.__version__)\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os, shutil\n",
    "from shutil import copyfile\n",
    "import copy\n",
    "from PIL import Image\n",
    "import csv\n",
    "import random\n",
    "# from models.imagenet import mobilenetv2\n",
    "print(\"All necessary modules imported.\")\n",
    "\n",
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'mobilenet_v2', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stdout = open(\"output.txt\", \"w\")\n",
    "\n",
    "###\n",
    "\n",
    "# sys.stdout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### REMARK: the syntax is very different for a regular Python script ###\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "# from tqdm import tnrange, tqdm_notebook\n",
    "# from time import sleep\n",
    "# for i in tqdm.notebook.tnrange(4, desc='1st loop'):\n",
    "#     for j in tqdm.notebook.tnrange(100, desc='2nd loop'):\n",
    "#         sleep(0.01)\n",
    "# print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect if we have a GPU available\n",
    "print(\"Cuda available: \" + str(torch.cuda.is_available()))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workaround because computing nodes cannot access the internet\n",
    "# loads the appropriate pretrained model from the Torchvision library\n",
    "# models pretrained on the ImageNet dataset \n",
    "def load_model(model_Name, num_classes, feature_extract, dict_path=None):\n",
    "    print(\"dict_path is None: \" + str(dict_path==None))\n",
    "    if model_Name == \"SqueezeNet\":\n",
    "        model = torchvision.models.squeezenet1_0(pretrained=False)\n",
    "        PATH = dict_path + \"/squeezenet1_0-a815701f.pth\"\n",
    "        if (dict_path != None):\n",
    "            model.load_state_dict(torch.load(PATH))\n",
    "            print(\"Pretrained model successfully loaded.\")\n",
    "#         set_parameters_that_require_grad(model, feature_extract)\n",
    "        set_parameters_that_require_grad(model, feature_extract)\n",
    "        model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model.num_classes = num_classes\n",
    "        input_size = 224\n",
    "    elif model_Name == \"mobilenet_v2\":\n",
    "        model = torchvision.models.mobilenet_v2(pretrained=False)\n",
    "        PATH = dict_path + \"/mobilenet_v2-b0353104.pth\"\n",
    "        if (dict_path != None):\n",
    "            model.load_state_dict(torch.load(PATH))\n",
    "            print(\"Pretrained model successfully loaded.\")\n",
    "        set_parameters_that_require_grad(model, feature_extract)        \n",
    "        model.classifier[1] = nn.Linear(1280, num_classes)\n",
    "        model.num_classes = num_classes\n",
    "        input_size = 224\n",
    "    elif model_Name == \"resnet50\":\n",
    "        model = torchvision.models.resnet50(pretrained=False)\n",
    "        PATH = dict_path + \"/resnet50-19c8e357.pth\"\n",
    "        if (dict_path != None):\n",
    "            model.load_state_dict(torch.load(PATH))\n",
    "            print(\"Pretrained model successfully loaded.\")\n",
    "        set_parameters_that_require_grad(model, feature_extract)\n",
    "        # reshaping the network\n",
    "        num_in_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_in_features, num_classes) # adding Linear layer at the end\n",
    "        input_size = 224\n",
    "    elif model_Name == \"inception_v3\": ## doesn't load!\n",
    "#       Inception v3\n",
    "#       Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        model = torchvision.models.inception_v3(pretrained=False)\n",
    "        PATH = dict_path + \"/inception_v3_google-1a9a5a14.pth\"\n",
    "        print(\"Starting to load...\")\n",
    "        if (dict_path != None):\n",
    "            model.load_state_dict(torch.load(PATH))\n",
    "            print(\"Pretrained model successfully loaded.\")\n",
    "        input_size = 299\n",
    "#     elif model_Name == \"Xception\":\n",
    "# #       Inception v3\n",
    "# #       Be careful, expects (299,299) sized images and has auxiliary output\n",
    "#         model = torchvision.models.xception(pretrained=False)\n",
    "#         PATH = dict_path + \"/inception_v3_google-1a9a5a14.pth\"\n",
    "#         print(\"Starting to load...\")\n",
    "#         if (dict_path != None):\n",
    "#             model.load_state_dict(torch.load(PATH))\n",
    "#             print(\"Pretrained model successfully loaded.\")\n",
    "#         input_size = 299\n",
    "    elif model_Name == \"vgg11_bn\":\n",
    "        model = torchvision.models.vgg11_bn(pretrained=False)\n",
    "        PATH = dict_path + \"/vgg11_bn-6002323d.pth\"\n",
    "        if (dict_path != None):\n",
    "            model.load_state_dict(torch.load(PATH))\n",
    "            print(\"Pretrained model successfully loaded.\")\n",
    "        set_parameters_that_require_grad(model, feature_extract)\n",
    "        num_in_features = model.classifier[6].in_features\n",
    "        model.classifier[6] = nn.Linear(num_in_features, num_classes)\n",
    "        input_size = 224\n",
    "    else:\n",
    "        raise Exception(\"Model not recognized. Exiting.\")\n",
    "    return model, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameters_that_require_grad(model, feature_extract):\n",
    "    if feature_extract: # if we're in feature extracting mode\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False # freeze pretrained model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function loading labeled dataset (expecting labelling structure)\n",
    "def load_dataset(data_path, transforms):\n",
    "    data_set = ImageFolder(data_path, transforms)\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.transform:\n",
    "            x = self.transform(dataset[index][0])\n",
    "        else:\n",
    "            x = dataset[index][0]\n",
    "        y = dataset[index][1]\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "\n",
    "        return len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  training function  ##\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    valResultsPerEpoch = {} # epoch number is the key and the corresponding value is a list of absolute values\n",
    "                            # of the differences between real label and predicted label for all images in the \n",
    "                            # validation set\n",
    "    \n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        print(\"epoch #\", epoch)\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10) \n",
    "        \n",
    "        valResultsPerEpoch[epoch] = []\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in tqdm(['train', 'val']):\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            # Iterate over data.\n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an\n",
    "                    # auxiliary output. In train mode we calculate the loss by\n",
    "                    # summing the final output and the auxiliary output but in\n",
    "                    # testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        print(\"(Not Inception)\")\n",
    "                        outputs = model(inputs)\n",
    "                        \n",
    "                        ####\n",
    "#                         if phase == 'val':\n",
    "#                             print(\"outputs\")\n",
    "#                             print(outputs)\n",
    "#                             print(\"length of outputs: \", len(outputs))\n",
    "                        ####\n",
    "                        \n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    \n",
    "                    ####\n",
    "                    if phase == 'val':\n",
    "                        print(\"preds-labels:\")\n",
    "                        diff = torch.abs(preds-labels) # returns a tensor! of absolute value of differences\n",
    "                        diff_list = diff.tolist() # now a list\n",
    "                        valResultsPerEpoch[epoch].extend(diff_list)\n",
    "                        print(\"DONE.\")\n",
    "                    ####\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                # statistics\n",
    "                # addition for each dataloader batch\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "                \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_epoch = epoch\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "                \n",
    "        print()\n",
    "        \n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best epoch was epoch #', best_epoch)\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))   \n",
    "    \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history, valResultsPerEpoch, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.IntTensor([-2,  0,  3,  5,  0,  3,  5, -5, -1, -2,  2, -2,  2,  0,  0, -3,  1,  0,\n",
    "        -7,  3,  3,  1,  1,  0,  0,  1,  1,  0,  0, -2,  3, -7,  1, -1, -5,  1,\n",
    "        -3, -1, -3,  0,  5,  0, -3,  0, -1,  3,  0, -1, -1,  0,  1, -6, -1,  0,\n",
    "        -1,  1,  0, -2, -6, -1,  1, -3, -7,  1], device='cpu')\n",
    "# print(x)\n",
    "y = torch.abs(torch.IntTensor([-2,  0, -7, -3, -3,  0,  0, -7,  0, -1, -3,  0, -1, -2,  0, -1, -2, -1,\n",
    "         5, -1, -2,  0,  0, -4, -1,  0, -1,  0, -1, -7,  0,  2, -1,  0,  1,  1,\n",
    "        -4, -1, -2, -1, -1,  0, -1,  0,  0, -2, -1, -1,  1,  0, -2, -2, -6,  1,\n",
    "        -7,  1,  0,  0,  0, -3, -6, -1,  3,  0], device='cpu'))\n",
    "# print(y)\n",
    "z = torch.abs(torch.IntTensor([ 3,  0, -4, -4,  0,  1,  1, -2,  0, -1,  0, -2, -1, -2,  0,  2, -1,  3,\n",
    "         0,  0, -1,  0, -3,  0,  4,  6, -1,  2,  0, -4,  0, -1,  2,  3,  0,  2,\n",
    "         2, -5, -3, -1, -1, -3, -2, -1, -1,  0,  1,  0,  0, -1, -1, -3,  3,  5,\n",
    "         0,  2,  0, -7, -1, -2,  2,  5,  0,  1], device='cpu'))\n",
    "\n",
    "# print(torch.abs(x-y))\n",
    "x = torch.abs(x)\n",
    "print(x)\n",
    "print(x.shape)\n",
    "x_list = x.tolist()\n",
    "y_list = y.tolist()\n",
    "x_list.extend(y_list)\n",
    "print(x_list)\n",
    "print(len(x_list))\n",
    "print(\"----\")\n",
    "myDict = {1:[1,2]}\n",
    "print(myDict)\n",
    "print(myDict[1])\n",
    "myDict[2] = []\n",
    "print(myDict)\n",
    "myDict[2].extend([2,7])\n",
    "print(myDict)\n",
    "myDict[2].extend([2,7])\n",
    "print(myDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE !! ## --> images present in GoogleStreetView_images/labelled_data_already_scp-ed/0 were COPIED from\n",
    "## 0_full and hence can be removed without losing data. \n",
    "# so when adding new data, remove the label 0 folder in labelled_data_already_scp-ed/0\n",
    "\n",
    "### RUN 1st before datasetMerge ###\n",
    "### iterates through the files contained in rootDir and deletes '.DS_Store' bad files ###\n",
    "### then finds the largest index, necessary for appropriate merging of the datasets   ###\n",
    "def DS_StoreCleanup():\n",
    "    notebook_path = os.path.dirname(os.path.realpath('__file__'))\n",
    "    rootDir = notebook_path + '/GoogleStreetView_images/labelled_data_already_scp-ed/'\n",
    "    count = 0\n",
    "    subDirs = os.listdir(rootDir)\n",
    "    print(subDirs)\n",
    "    arr = [] # array of \"good\" filenames\n",
    "    for sub in subDirs:\n",
    "        files = os.listdir(rootDir + sub)\n",
    "        count += len(files)\n",
    "        for f in files:\n",
    "            absfilepath = rootDir + sub + '/' + f\n",
    "            if f == '.DS_Store':\n",
    "                os.remove(absfilepath)\n",
    "                count -= 1\n",
    "                print(\"Useless file deleted.\")\n",
    "            else:\n",
    "                arr.append(int(f[:-4]))\n",
    "    print(count, \" labeled images in dataset\")\n",
    "    \n",
    "#     print(len(os.listdir(rootDir + '../0_full')))\n",
    "## also count the indexes (filenames) in 0_full\n",
    "    for f in os.listdir(rootDir + '../0_full'):\n",
    "        arr.append(int(f[:-4]))\n",
    "    \n",
    "    print(\"array size: \", len(arr))\n",
    "    # print(arr)\n",
    "    maxIndex = max(arr) \n",
    "    print(\"Max index: \", maxIndex) \n",
    "    return maxIndex\n",
    "    ### when I add more images, start k at this count value !! ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### merges the two datasets (labelled_data merged into labelled_data_already_scp-ed) ###\n",
    "### renaming and moving the files using maxIndex to avoid naming issues              ### \n",
    "def datasetMerge(maxIndex):\n",
    "    notebook_path = os.path.dirname(os.path.realpath('__file__'))\n",
    "    print(notebook_path)\n",
    "    rootDir = notebook_path + '/GoogleStreetView_images/labelled_data/' ## new, just added (via recent SCP) labelled data\n",
    "    parentDir = notebook_path + '/GoogleStreetView_images/'\n",
    "    print(rootDir)\n",
    "    count = 0\n",
    "    subDirs = os.listdir(rootDir)\n",
    "    print(subDirs)\n",
    "    for sub in subDirs:  # iterating through ['2', '5', '1', '6', '4', '3', '7', '0']\n",
    "        files = os.listdir(rootDir + sub)\n",
    "        for file in files:\n",
    "            # renames & moves the photo\n",
    "    #         print(rootDir + sub + '/' + file)\n",
    "    #         print(parentDir + 'labelled_data_already_scp-ed/' + sub + '/' + str(maxIndex+1) +'.jpg')\n",
    "            if file == '.DS_Store':\n",
    "                print(\"do nothing\")\n",
    "            else:\n",
    "                os.rename(rootDir + sub + '/' + file, \n",
    "                          parentDir + 'labelled_data_already_scp-ed/' + sub + '/' + str(maxIndex+1) + '.jpg') \n",
    "                maxIndex += 1 # increment to never run into \"2 files with same name\" issues\n",
    "\n",
    "    print(\"DONE\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function returns the number of files in the subdirectory structure ###\n",
    "def howManyFiles(path):\n",
    "    notebook_path = os.path.dirname(os.path.realpath('__file__'))\n",
    "    rootDir = notebook_path + path\n",
    "    subDirs = os.listdir(rootDir)\n",
    "    count = 0\n",
    "    for sub in subDirs:\n",
    "        files = os.listdir(rootDir + sub)\n",
    "        count += len(files)\n",
    "    print(path, \" contains \", count, \" images.\")\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "howManyFiles('/GoogleStreetView_images/labelled_data_already_scp-ed/')\n",
    "# howManyFiles('/GoogleStreetView_images/labelled_data/')\n",
    "# maxIndex = DS_StoreCleanup()\n",
    "# print(\"Max Index: \", maxIndex) \n",
    "# datasetMerge(maxIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###              CODE TO DOWNSAMPLE label 0 images              ###\n",
    "###  by moving the folder elsewhere & randomly selecting files  ###\n",
    "def downSample1():\n",
    "    notebook_path = os.path.dirname(os.path.realpath('__file__'))\n",
    "    streetViewData_dir = \"/GoogleStreetView_images/new_test_set-never_seen\"\n",
    "    path = notebook_path + streetViewData_dir\n",
    "    dirs = os.listdir(path)\n",
    "    dirs.sort()\n",
    "    totalNumImages = 0\n",
    "    sizesPerLabel = {}\n",
    "    for k in dirs:\n",
    "        path2 = path + '/' + k\n",
    "        files = os.listdir(path2)\n",
    "    #     print(k, \" \", len(files))\n",
    "        sizesPerLabel[k] = len(files)\n",
    "        totalNumImages += len(files)\n",
    "    print(sizesPerLabel)\n",
    "    print(\"Total number of images in the directory: \", totalNumImages)\n",
    "    print(\"---\")\n",
    "\n",
    "    # calculate average of number of images for labels 1(!!) to 7\n",
    "    sum = 0\n",
    "    for k in range(1,7+1):\n",
    "#         print(k)\n",
    "        path2 = path + '/' + str(k)\n",
    "        sum += len(os.listdir(path2))\n",
    "    print(sum, \" images of labels 1 to 7\")\n",
    "#     print(sum == (totalNumImages-sizesPerLabel['0']))\n",
    "    average = sum//7\n",
    "    print(\"Average # of images for labels 1 to 7: \", average)\n",
    "    return path, average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   SUITE   ###\n",
    "# to be ran after the previous block #\n",
    "# \n",
    "def downSample2(path, average):\n",
    "    print(\"Average is \", average, \" images\")\n",
    "    source = path + \"/0\"\n",
    "    print(source)\n",
    "    destination = path + \"/../0_full_new\"\n",
    "    print(destination) \n",
    "    if os.path.isdir(destination):\n",
    "        print(\"0_full folder exists\")\n",
    "        print(\"0_full folder contains \", len(os.listdir(destination)), \" images\")\n",
    "    print(\"(new) label0 folder contains \", len(os.listdir(source)), \" images\")\n",
    "    \n",
    "#     # moves folder 0 images to 0_full (if there're files with same name, overwrites!)\n",
    "#     for file in os.listdir(source):\n",
    "#         os.rename(source + '/' + file, destination + '/' + file)\n",
    "#     print(len(os.listdir(destination)), \" images in 0_full folder\")\n",
    "    \n",
    "    \n",
    "    # extract a random sublist from 0_full\n",
    "    subList = random.sample(os.listdir(destination), average)\n",
    "#     print(subList)\n",
    "    print(len(subList))\n",
    "    \n",
    "    # copy the randomly selected files\n",
    "    for k in subList:\n",
    "        copyfile(destination + \"/\" + k, source + \"/\" + k) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path, average = downSample1()\n",
    "downSample2(path, average)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_it(hist, num_epochs):\n",
    "    ohist = []\n",
    "    ohist = [h.cpu().numpy() for h in hist]\n",
    "    \n",
    "    plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
    "    plt.xlabel(\"Training Epochs\")\n",
    "    plt.ylabel(\"Validation Accuracy\")\n",
    "    plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "    plt.ylim((0,1.))\n",
    "    plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "    plt.legend()\n",
    "    plt.savefig('validation_accuracy_graph.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # proxy of the main function to be implemented\n",
    "    phoneData_dir = \"./Rogers_Corpus\"\n",
    "    data360_dir   = \"./CV-Aid-for-Visually-Impaired/data_processing/labelled_360/labelled_data\"\n",
    "    streetViewData_dir = \"./GoogleStreetView_images/labelled_data_already_scp-ed\"\n",
    "\n",
    "    # Ratio of training data to validation data\n",
    "    train_ratio = 0.9\n",
    "\n",
    "    # Number of classes in the dataset (class 0 being unknown)\n",
    "    num_classes = 8 # for google street view data\n",
    "\n",
    "    # Batch size for training (change depending on how much memory you have)\n",
    "    batch_size = 64\n",
    "\n",
    "    # Number of epochs to train for\n",
    "    num_epochs = 20\n",
    "\n",
    "    # Flag for feature extracting. When False, we finetune the whole model,\n",
    "    # when True we only update the reshaped (final) layer params\n",
    "    feature_extract = False\n",
    "\n",
    "    # Hyperparameters for models (learning rate and momentum)\n",
    "    learning_rate = 0.001\n",
    "    momentum = 0.9\n",
    "\n",
    "    # enter desired pretrained model\n",
    "    modelName = \"mobilenet_v2\"\n",
    "    # modelX = init_Model(modelName, num_classes, feature_extract, use_pretrained=True)\n",
    "    # print(modelX)\n",
    "\n",
    "\n",
    "    modelX, input_size = load_model(modelName, num_classes, feature_extract, \"./raw_pretrained_models\")\n",
    "    # print(model)   #--works\n",
    "\n",
    "#     print(modelX)\n",
    "    print(\"Model imported.\")\n",
    "    \n",
    "  ##############\n",
    "\n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        # normalize using ImageNet's mean & standard deviation values\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    print(\"Initializing Datasets and Dataloaders...\")\n",
    "    \n",
    "    # creating training and validation datasets\n",
    "#     labeled_dataset = LabeledDataset(transform=data_transform)\n",
    "    \n",
    "    print(\"input size: \", input_size)\n",
    "\n",
    "#     data_set_360 = load_dataset(data360_dir, data_transform) # 9 labels\n",
    "#     print(data_set_360.classes)\n",
    "#     print(data_set_360[1])\n",
    "#     print(\"sizeee: \", len(data_set_360[1]), \"=====\")\n",
    "#     print(data_set_360[0][0].shape)  ## returns torch.Size([3, 224, 224]) as desired\n",
    "    \n",
    "#     phoneCamera_data_set = load_dataset(phoneData_dir, data_transform) # 8 labels\n",
    "#     print(phoneCamera_data_set.classes)\n",
    "    \n",
    "    streetView_data_set = load_dataset(streetViewData_dir, data_transform) # 8 labels\n",
    "    print(\"-----------\")\n",
    "    print(streetView_data_set)\n",
    "    print(streetView_data_set.class_to_idx)\n",
    "#     print(  streetView_data_set['0']      )\n",
    "    \n",
    "    \n",
    "#     print(\"dataset360 size: \", len(data_set_360)) # adds up to the number of images --> 9745\n",
    "#     print(\"datasetPhoneCamera size: \", len(phoneCamera_data_set))\n",
    "    print(\"streetView_data_set size: \", len(streetView_data_set))\n",
    "#     x,y = data_set_360[0]\n",
    "#     print(x,y)\n",
    "    \n",
    "#     train_size360 = int(train_ratio * len(data_set_360))\n",
    "#     val_size360  = len(data_set_360) - train_size360\n",
    "#     train_sizeCamera = int(train_ratio * len(phoneCamera_data_set))\n",
    "#     val_sizeCamera  = len(phoneCamera_data_set) - train_sizeCamera\n",
    "    train_sizeStreetView = int(train_ratio * len(streetView_data_set))\n",
    "    val_sizeStreetView = len(streetView_data_set) - train_sizeStreetView\n",
    "    \n",
    "#     training_set360, val_set360 = torch.utils.data.random_split(data_set_360, [train_size360, val_size360])\n",
    "#     training_setCamera, val_setCamera = torch.utils.data.random_split(phoneCamera_data_set, \n",
    "#                                                                        [train_sizeCamera, val_sizeCamera])\n",
    "    training_StreetView, val_StreetView = torch.utils.data.random_split(streetView_data_set,\n",
    "                                                                        [train_sizeStreetView, val_sizeStreetView])\n",
    "    \n",
    "#     print(\"length training_set360: \", len(training_set360), \" images\")\n",
    "#     print(type(training_set360))\n",
    "#     print(\"length val_set360: \",len(val_set360), \" images\")\n",
    "#     print(type(val_set360))\n",
    "#     print(\"length training_setCamera: \", len(training_setCamera), \" images\")\n",
    "#     print(type(training_setCamera))\n",
    "#     print(\"length val_setCamera: \",len(val_setCamera), \" images\")\n",
    "#     print(type(val_setCamera))\n",
    "    print(\"length training_StreetView: \", len(training_StreetView), \" images\")\n",
    "    print(type(training_StreetView))\n",
    "    print(\"length val_StreetView: \",len(val_StreetView), \" images\")\n",
    "    print(type(val_StreetView))\n",
    "    \n",
    "        \n",
    "#     image360_datasets = {'train': training_set360, 'val': val_set360 }\n",
    "#     imageCamera_datasets = {'train': training_setCamera, 'val': val_setCamera }\n",
    "    imageStreetView_datasets = {'train': training_StreetView, 'val': val_StreetView }\n",
    "    \n",
    "    # load the data into batches\n",
    "#     dataloaders360_dict = {x: DataLoader(image360_datasets[x], batch_size=batch_size, shuffle=True)\n",
    "#                        for x in ['train', 'val']}\n",
    "    \n",
    "#     dataloadersCamera_dict = {x: DataLoader(imageCamera_datasets[x], batch_size=batch_size, shuffle=True)\n",
    "#                        for x in ['train', 'val']}\n",
    "    \n",
    "    dataloadersStreetView_dict = {x: DataLoader(imageStreetView_datasets[x], batch_size=batch_size, shuffle=True) \n",
    "                                  for x in ['train', 'val']}\n",
    "\n",
    "    print(dataloadersStreetView_dict['train'])\n",
    "    print(\"length of trainStreetView dataloader: \", len(dataloadersStreetView_dict['train']), \" batches\") \n",
    "    \n",
    "    modelX = modelX.to(device)\n",
    "#     return\n",
    "    #  Gather the parameters to be optimized/updated in this run. If we are\n",
    "    #  finetuning we will be updating all parameters. However, if we are\n",
    "    #  doing feature extract method, we will only update the parameters\n",
    "    #  that we have just initialized, i.e. the parameters with requires_grad\n",
    "    #  is True.\n",
    "    params_to_update = modelX.parameters()\n",
    "#     print(\"Parameters to learn:\")\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "        for name, param in modelX.named_parameters():\n",
    "            if param.requires_grad is True:\n",
    "                params_to_update.append(param)\n",
    "                print(\"\\t\", name)\n",
    "    else:\n",
    "        for name, param in modelX.named_parameters():\n",
    "            if param.requires_grad is True:\n",
    "                print(\"\\t\", name)\n",
    "\n",
    "    # Observe that all parameters are being optimized\n",
    "    optimizerX = optim.SGD(params_to_update, lr=learning_rate, momentum=momentum)\n",
    "    \n",
    "    # Setup the loss fxn\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Train and evaluate\n",
    "    modelX, hist, valResultsPerEpoch, bestEpoch = train_model(modelX, dataloadersStreetView_dict, criterion,\n",
    "                                 optimizerX, num_epochs=num_epochs,\n",
    "                                 is_inception=(modelName == \"inception\"))\n",
    "    ## modelX is now the best trained model\n",
    "    \n",
    "    print(\"best epoch was: \", bestEpoch)\n",
    "    print(\"valResultsPerEpoch for this epoch:\")\n",
    "    print(valResultsPerEpoch[bestEpoch])\n",
    "    \n",
    "    torch.save(modelX, \"./bestModel-Jul09.pth\")\n",
    "\n",
    "    \n",
    "    plot_it(hist, num_epochs)\n",
    "     \n",
    "#     sys.stdout.close()\n",
    "#     plot_it(hist, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(modelX, \"model\")      \n",
    "# plot_it(hist, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check best trained model on a new random validation set ###\n",
    "streetViewData_dir = \"./GoogleStreetView_images/labelled_data_already_scp-ed\"\n",
    "\n",
    "# Ratio of training data to validation data\n",
    "train_ratio = 0.9\n",
    "\n",
    "# Number of classes in the dataset (class 0 being unknown)\n",
    "num_classes = 8 # for google street view data\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 64\n",
    "input_size = 224\n",
    "\n",
    "notebook_path = os.path.dirname(os.path.realpath('__file__'))\n",
    "path = notebook_path + '/bestModel-Jul09.pth'\n",
    "print(path)\n",
    "# model = torch.load(path, map_location='cpu')\n",
    "model2_7 = torch.load(path)\n",
    "print(model)\n",
    "print(\"Model imported.\")\n",
    "\n",
    "##############\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(input_size),\n",
    "    transforms.ToTensor(),\n",
    "    # normalize using ImageNet's mean & standard deviation values\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "print(\"input size: \", input_size)\n",
    "\n",
    "streetView_data_set = load_dataset(streetViewData_dir, data_transform) # 8 labels\n",
    "print(\"-----------\")\n",
    "print(streetView_data_set)\n",
    "print(streetView_data_set.class_to_idx)\n",
    "\n",
    "print(\"streetView_data_set size: \", len(streetView_data_set))\n",
    "\n",
    "train_sizeStreetView = int(train_ratio * len(streetView_data_set))\n",
    "val_sizeStreetView = len(streetView_data_set) - train_sizeStreetView\n",
    "\n",
    "training_StreetView, val_StreetView = torch.utils.data.random_split(streetView_data_set,\n",
    "                                                                    [train_sizeStreetView, val_sizeStreetView])\n",
    "\n",
    "print(\"length training_StreetView: \", len(training_StreetView), \" images\")\n",
    "print(type(training_StreetView))\n",
    "print(\"length val_StreetView: \",len(val_StreetView), \" images\")\n",
    "print(type(val_StreetView))\n",
    "\n",
    "imageStreetView_datasets = {'train': training_StreetView, 'val': val_StreetView }\n",
    "\n",
    "\n",
    "dataloadersStreetView_dict = {x: DataLoader(imageStreetView_datasets[x], batch_size=batch_size, shuffle=True) \n",
    "                              for x in ['train', 'val']}\n",
    "\n",
    "print(dataloadersStreetView_dict['train'])\n",
    "print(\"length of trainStreetView dataloader: \", len(dataloadersStreetView_dict['train']), \" batches\")\n",
    "print(\"length of valStreetView dataloader: \", len(dataloadersStreetView_dict['val']), \" batches\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check best trained model on a new random validation set ###\n",
    "arr_preds = []\n",
    "arr_labels = []\n",
    "k = 0\n",
    "running_corrects = 0\n",
    "model2_7.eval()\n",
    "\n",
    "for inputs, labels in dataloadersStreetView_dict['val']:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = model2_7(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        arr_preds.append(preds.tolist())\n",
    "        arr_labels.append(labels.tolist())\n",
    "\n",
    "        if k < 3:\n",
    "            print(\"predictions:\", preds)\n",
    "            print(\"labels: \", labels)\n",
    "        k += 1\n",
    "\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "print(\"good predictions: \", running_corrects)\n",
    "accuracy = running_corrects.double() / len(dataloadersStreetView_dict['val'].dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check best trained model on a new random validation set ###\n",
    "accuracy = running_corrects.double() / len(dataloadersStreetView_dict['val'].dataset)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check best trained model on a new random validation set ###\n",
    "arr_preds_bis = []\n",
    "arr_labels_bis = [] \n",
    "print(len(arr_preds[0]))\n",
    "print(len(arr_labels))\n",
    "print(\"**\")\n",
    "# print(arr_preds[0])\n",
    "print(\"**\")\n",
    "# print(arr_preds)\n",
    "print(arr_labels)\n",
    "# print(arr_preds[0][1])\n",
    "print(\"----\")\n",
    "for k in range(10):\n",
    "    for i in range(len(arr_preds[k])):\n",
    "        arr_preds_bis.append(arr_preds[k][i])\n",
    "\n",
    "for k in range(10):\n",
    "    for i in range(len(arr_labels[k])):\n",
    "        arr_labels_bis.append(arr_labels[k][i])\n",
    "# print(arr_preds_bis)\n",
    "print(arr_labels_bis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check best trained model on a new random validation set ###\n",
    "notebook_path = os.path.dirname(os.path.realpath('__file__'))\n",
    "path = notebook_path + '/bestModel-Jul09.pth'\n",
    "print(path)\n",
    "model = torch.load(path)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "dummy_input = torch.randn(1, 3, 224, 224, device=device)\n",
    "# print(dummy_input)\n",
    "input_names = [\"input\"]\n",
    "output_names = [\"output\"]\n",
    "torch.onnx.export(model, dummy_input, \"bestModel2_onnx.onnx\", verbose=True, \n",
    "                  input_names=input_names, output_names=output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
